#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°æ¶æ„è¯æ®ç”Ÿæˆä¸“ç”¨è„šæœ¬
ä¸“é—¨ç”¨äºè¿è¡Œæ–°æ¶æ„çš„è¯æ®ç”Ÿæˆï¼Œæ¯ä¸ªè¯æ®ç‹¬ç«‹æ–‡ä»¶
"""

import sys
import json
from pathlib import Path

# æ·»åŠ srcåˆ°è·¯å¾„
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

from loguru import logger
from src.services.evidence_file_generator import EvidenceFileGenerator
from src.services.stage0.stage0_service import Stage0Service
from src.services.stage1.stage1_service import Stage1Service
from src.utils import LLMClient

def run_new_architecture_generation():
    """è¿è¡Œæ–°æ¶æ„è¯æ®ç”Ÿæˆ"""
    logger.info("ğŸš€ å¼€å§‹æ–°æ¶æ„è¯æ®ç”Ÿæˆæµç¨‹")
    
    try:
        # æ­¥éª¤1: è¿è¡Œé˜¶æ®µ0åˆ†æï¼ˆå¦‚æœè¿˜æ²¡æœ‰ç»“æœï¼‰
        stage0_dir = Path("outputs/stage0")
        if not (stage0_dir / "0.5_evidence_planning.json").exists():
            logger.info("ğŸ“‹ æ‰§è¡Œé˜¶æ®µ0åˆ†æ...")
            stage0_service = Stage0Service()
            
            # ä½¿ç”¨æµ‹è¯•åˆ¤å†³ä¹¦
            judgment_path = Path("æµ‹è¯•ç”¨åˆ¤å†³ä¹¦/(2024)æ²ª74æ°‘åˆ245å·.pdf")
            if not judgment_path.exists():
                # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
                judgment_path = Path("/Users/liuzhen/Documents/æ²³å¹¿/Product Development/chatGPT/Digital Law/Digital court/é‡‘èæ³•é™¢/æ³•å®˜æ•°å­—åŠ©æ‰‹/æ¡ˆå·ææ–™æ ·ä¾‹/èèµ„ç§Ÿèµ/(2024)æ²ª74æ°‘åˆ721å·/OpenCode Trial/æµ‹è¯•ç”¨åˆ¤å†³ä¹¦/(2024)æ²ª74æ°‘åˆ245å·.pdf")
            
            if judgment_path.exists():
                stage0_result = stage0_service.run_all(str(judgment_path))
                logger.info("âœ… é˜¶æ®µ0åˆ†æå®Œæˆ")
            else:
                logger.error("âŒ æœªæ‰¾åˆ°åˆ¤å†³ä¹¦æ–‡ä»¶")
                return False
        else:
            logger.info("ğŸ“‹ é˜¶æ®µ0æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ†æ")
        
        # æ­¥éª¤2: æ„å»ºå®Œæ•´çš„é˜¶æ®µ0æ•°æ®ç»“æ„
        logger.info("ğŸ“‚ åŠ è½½é˜¶æ®µ0æ•°æ®...")
        stage0_data = {
            "0.1_structured_extraction": json.loads((stage0_dir / "0.1_structured_extraction.json").read_text()),
            "0.2_anonymization_plan": json.loads((stage0_dir / "0.2_anonymization_plan.json").read_text()),
            "0.3_transaction_reconstruction": json.loads((stage0_dir / "0.3_transaction_reconstruction.json").read_text()),
            "0.4_key_numbers": json.loads((stage0_dir / "0.4_key_numbers.json").read_text()),
            "0.5_evidence_planning": json.loads((stage0_dir / "0.5_evidence_planning.json").read_text())
        }
        
        logger.info(f"âœ… é˜¶æ®µ0æ•°æ®åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(stage0_data)} ä¸ªéƒ¨åˆ†")
        
        # æ­¥éª¤3: ä½¿ç”¨æ–°æ¶æ„ç”Ÿæˆè¯æ®æ–‡ä»¶
        logger.info("ğŸ“„ å¼€å§‹æ–°æ¶æ„è¯æ®ç”Ÿæˆ...")
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        evidence_output_dir = Path("outputs/stage1/evidence_new")
        evidence_output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–è¯æ®ç”Ÿæˆå™¨
        evidence_generator = EvidenceFileGenerator(
            prompt_dir="prompts",
            output_dir=str(evidence_output_dir),
            llm_client=LLMClient()  # ä½¿ç”¨é»˜è®¤é…ç½®
        )
        
        # ç”Ÿæˆæ‰€æœ‰è¯æ®æ–‡ä»¶ï¼ˆä½¿ç”¨æ–°æ¶æ„ï¼‰
        evidence_index = evidence_generator.generate_all_evidence_files(
            stage0_data=stage0_data,
            evidence_planning=stage0_data["0.5_evidence_planning"],
            party="åŸå‘Š"
        )
        
        # ä¿å­˜è¯æ®ç´¢å¼•
        evidence_index_path = evidence_output_dir / "evidence_index.json"
        with open(evidence_index_path, 'w', encoding='utf-8') as f:
            json.dump(evidence_index, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… æ–°æ¶æ„è¯æ®ç”Ÿæˆå®Œæˆ!")
        logger.info(f"   æ€»è¯æ®æ•°: {evidence_index.get('è¯æ®æ€»æ•°', 0)} ä¸ª")
        logger.info(f"   è¯æ®ç»„æ•°: {evidence_index.get('è¯æ®ç»„æ•°', 0)} ç»„")
        logger.info(f"   è¯æ®ç´¢å¼•: {evidence_index_path}")
        logger.info(f"   è¯æ®æ–‡ä»¶: {evidence_output_dir}/evidence/")
        
        # éªŒè¯ç”Ÿæˆç»“æœ
        evidence_files = list(evidence_output_dir.glob("evidence/**/*.txt"))
        logger.info(f"   å®é™…ç”Ÿæˆæ–‡ä»¶æ•°: {len(evidence_files)} ä¸ª")
        
        if len(evidence_files) == evidence_index.get('è¯æ®æ€»æ•°', 0):
            logger.info("âœ… æ–‡ä»¶æ•°é‡éªŒè¯é€šè¿‡")
        else:
            logger.warning("âš ï¸  æ–‡ä»¶æ•°é‡éªŒè¯å¤±è´¥")
        
        # æ˜¾ç¤ºéƒ¨åˆ†æ–‡ä»¶ç»“æ„
        logger.info("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶ç»“æ„é¢„è§ˆ:")
        for i, file_path in enumerate(evidence_files[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            relative_path = file_path.relative_to(evidence_output_dir)
            logger.info(f"   {relative_path}")
        if len(evidence_files) > 5:
            logger.info(f"   ... è¿˜æœ‰ {len(evidence_files) - 5} ä¸ªæ–‡ä»¶")
        
        logger.info("ğŸ‰ æ–°æ¶æ„è¯æ®ç”Ÿæˆæµç¨‹å®Œæˆ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ–°æ¶æ„è¯æ®ç”Ÿæˆå¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ”§ é‡‘èæ¡ˆä»¶æµ‹è¯•æ•°æ®ç”Ÿæˆç³»ç»Ÿ - æ–°æ¶æ„ä¸“ç”¨è¿è¡Œè„šæœ¬")
    logger.info("===============================================")
    
    success = run_new_architecture_generation()
    
    if success:
        logger.info("âœ… æ–°æ¶æ„è¿è¡ŒæˆåŠŸå®Œæˆ!")
        logger.info("ğŸ“„ è¯·æŸ¥çœ‹ä»¥ä¸‹è¾“å‡ºæ–‡ä»¶:")
        logger.info("   - outputs/stage1/evidence_new/evidence_index.json")
        logger.info("   - outputs/stage1/evidence_new/evidence/è¯æ®ç»„*/")
        logger.info("ğŸ“– è¯¦ç»†ä½¿ç”¨è¯´æ˜è¯·å‚è€ƒ: é‡‘èæ¡ˆä»¶æµ‹è¯•æ•°æ®ç”Ÿæˆæ–¹æ¡ˆ/æ–°è½¯ä»¶è¿è¡ŒæŒ‡å—.md")
    else:
        logger.error("âŒ æ–°æ¶æ„è¿è¡Œå¤±è´¥ï¼Œè¯·æŸ¥çœ‹é”™è¯¯ä¿¡æ¯")
        sys.exit(1)

if __name__ == "__main__":
    main()